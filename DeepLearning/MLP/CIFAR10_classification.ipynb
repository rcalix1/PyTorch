{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563b3367",
   "metadata": {},
   "source": [
    "\n",
    "## CIFAR10\n",
    "\n",
    "* classification (10 classes )\n",
    "* 60,000 training images (50k training; 10k testing)\n",
    "* RGB\n",
    "* 32x32 = ??\n",
    "* airplane (0); car (1), etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd21dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c463a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd63a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808189a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = 'data/cifar10data/'\n",
    "\n",
    "cifar10_training = datasets.CIFAR10(data_path, train=True, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a51ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a087b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(cifar10_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ecd474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(cifar10_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17644111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_train, label = cifar10_training[42]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8387bbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de88148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAHu0lEQVR4nCXTW3Nk11kG4Pc7rLV3d6sljUaemrFjnMQ4thOnIFSFmxSFoYq/BcVfg7twExeEFD7Edg6WR8OMpFYf9lrrO3DB8x8e+pd//te3ri4++eC902nsT+1/Pv/8+ulb29V0ebWJkEwM75SZNkY/IjGGlVKYOcL76BZxXI6v7+8AZhKijMj7u324//BHL1bTpJ9/+dX99TPxdnv7+v7uPqC3b/Zhtj2br66uV/Mc8N4GfBTx9VpEqPeT++hjJJw5y6xvv9iqKENUJQB755IFpYqb6Upxd3/3u+RxPCjG+uLi9f3+cDg8nk7f/um7H/zgxXLcf3d7e3d/f3V5/k+f/nJa67I8MvmqEosgQ5mKVkowkTAigyZBwIZPovqH77/dnJ2LeoaTnx7/9/H8/Gqa1ktbDvuHly8XpG3PdF6tN2upuj7sdiKjCGaZEMHEFExLMjGQHt0DARRRCdap6D/+w88EJJQqhVyGex+tFA1sklaiWUSUSyGl9M2m2ggKT8jwkZERniAKiKoIg5goVYqoJAXE9Ofv/uUY1r2BHQkWce/gGJ7dOjQYzC6zVMmAj3Xhs9V5d890ACQFTKpMxAzu3cxdRQXCKu5D79orEeYKFkoHIycVQUkl15kIRmZuzEmUp+OSMUhZKT2NiFnUza2FkBBBghkk7sKQkIQo1cwIIlJRUjCQ5mBNh8dgEDIKsTeDcOtxXuuKeMSwqMISlsIcSQwqIqAMZIefsiGTVfVJnDERRWIhUckMEJDkIK1T5ABLlTOl9e7xETlevznd3OwvrlZXl7OxWw4LZAEIiQyAiUEAkYApoOaRYQiIUKAYfO8nIaHY/P7Lm/3+KCwZhOSffPwxIH+6vfv8y2/Ot9uPPnj7ybVun0pIZlBhgWdJqiQqmpkAQ6FZ0zxBAIvzCIAX+f779rhv3/zh5avbl25mNl48f/H+xz+5ubnZHY5LX8br9uvdbrOe3//R9UefPAU0PBIgTYMHw9yLSiJ1SCNNAGCNDG/0+9/e37za3T22Yz+crPeliQjV6Yuv//hweDz1HhmegaRXD8v9bx5+9skzLWLB5u6ZHkEJMOARkRoZksjIY+5nnr/63Xf/8dm3dTPtD62NZQxj5nm1ftjv2p+/2T3ut+uz7fai9WE+IvtwGGi0pqpIINjNQM5FHJ4Z2lonouGdHFL4N599efvwsPbNbrcH4BHb9ZpYSbQPb62vVxLw/WlZlt3FRn75tx86EpkWHghBCUrmpExwIlnnuSrJcBYIg5+/c37z6s1hfwCwtCUymQjinplIIXt4c+vRp0mev3f9d7/66NnTbY8Boe4jkElgTTAFUWaCQzNpIABirkz5q08/vH7r8t/+/b+nebp7pJvbV7/4+bt/9Ysffv3Vndny4x//hXkiokdKTmeXfN92lESEQIpyYjCYlC2DichJYZwEIU5PzxiJ9z96/tl/fnNxtm2WpehPf/ru2+9c6ixff3V48d4V4BL65v70xRen9zpNosFE4AwvohamLMMsgST2dOUkjzCEMBVIpWo5Pv37vy7i82/Xu93D8+fXGHJ1vv2vx4fl3p882UaO68uzm40VrZwBuFDxJOuIxIIB4jFGKWWEKZQloSQgCDNHEpV33p3N2t+s19tzWc/VA0j/8IOnDw8topIurPLixfzy5eHsSbbWpjIJiw0PpMOUFeDTOAGp3RsRPNmsEVcJHdE9AMrA+ODjt+/bMTyJffOEb//s9fKemSQnXfvtt311ObvkghOBIPAIJrE0EQn3KkX78CRjUk3p1otkd4+IjCBFwyAJUSCozETs3WjepPmI6Kzz/g7lIsxDgpm48owEISQZjojUEZZAWJ9l6jGOdvr/86d2rFnnOrXWLEcic6E6b29vlrU/RiAyift3f5zq4XHaROXKycwjMkScqXqACGrZAVLl4OAEG9IpOGopw/owixxJmYkYTD5KaYeHRsqlFPA4eyLeN5GHqGqJaUpoRupwW2wgocN7Ip2YbAxvQgUSzEqicDCYqHiYQKIQT7HhKXImye5LlU3RRHJ3ikwVCQuHZZTMsZqrkysItVZK7qccvEjhqtUaMTDViQ2RVHUS40GjVB4UTFiV8zGOxHnoR5npbJqQHMbTVKydMjR0YpaRplf6VEioICScJhqVYTmzKPfeuWp6oFBQKhMzz1QqSYTOM3vYJDXAmckQR/blRK6iKVQ9oqSow4qwQihldO4t69lGoDkyB9dSgnGIRyGZ6opFyZMBIhnkcOJUzwFiC1vaYfGhMlXiWgSdqs6alqfeZp09BguhuCUSQZFF6NSbZ9S5qCgGeevdQ1MKhaeLSCInngJIpSJ0RSWYmAkBrIgZGokIO8QhU6hHLXJ43M3bszYOTBnBBsdpOvgRTmbxME7PNttOqes5Ee5hI0gZrggEJ3N2CxVNGKsqiNw9zJuFILvzsGgPOykIDubKKQZPSS3MQc94i6BGZKdodnQYPIiFAirF4riZziL8uCyp2vYPOtWpSkmL64vZRzghuQFZ54mYe7eqxdMtzGIwWFFPo7d+YhrunhlVarB37x6BQW702E4Nrda1e6p3ICHMh6VJApQjXavu2nGSeQxLygwnEotksuFOQkVoUJCKN3YubekEHc1nWgeljsmGM2Kt5/8HlP+YdNEO/dwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef512c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_test, label = cifar10_test[2345]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1eaafd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45445429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAIiUlEQVR4nC2Wy4+mx1XGz62q3st36e9r93RPd894nPHEHjs2jkMUCQUWIMguG1aIRdiHDRKLSPk7+CdYZIEEQoocCNngiIsNIzyDccbdzrSn3dfv9t6q6hwWn8+q6iye8ztPLerBn+ABGAi4GoqCnDMgTEi0M51OKn+7Xr9cNL2BWUYwAzRA+LoyISASGpABk9WjUAXvRQBxMi69s/UyC4FTyIaYTAcdiMQHFpF1twp16UfsWx+7BKAAgICIzMSqigYesPRlzFEphiDz3Xo68QQaYwxFIhY3mMywUsuDAQAoADONx2UofIwpmhLzZOQ8Yd+zmTFh8DAe1zHGlAbnKHjPXLAQUqpqP6m8aY4MLhCI5cRyUE8JLIpFTW3TMOW6LA0NvVMiAxuPcTwObaMAyEylxPGoAAtt7IHRzAgRchahIBT7RITBl9kyE9YjkrLkpP1oXDqs2lEw08qHVdcOloWYCcUBETiHRMSMJVNVSs7ZWlIgZjZVlIRgYKYGMaUhpqwqg0xmlaxjd7I4C205k3pUVvPJpB26NrZceCBlIXHoiLxjM0MEVxIVYhk9gykaGCGDgqkiASNjRjXlBMOiXRDJRb9eabzZxCV05eIWEO/fewVW2GkKjoMwO/BCTiSnDAhQAJbMikWWrh1MzUDJQEgAgJURCQAATSOefH4uX3VrQ0ByyVxWvLi63tuv53dmmzRIAjZ1HspxicGnlMzUFciMAEDAKJRiQgRGRQBmkeT6TR9jygTqvSYSJXM5sPqSi3nwwWLTrMvd0Wg6ddna20UG67MyGTtBMyTs+t55732oas4pIWGGZDF5EkQjRosQYzTM5Fg0J7LCAzu1vZ3pZCSju544e8o8KhoVimqgfbvp08DOcYc5JTekFjthZiIwEEZCspS7tos5D9n6QVMGcbUoKBJ4Ik/QazfdO7YwGJnzgix1PaYulU6Qoc+pGXpFDVUVQgBEVU0pMSEyCnPctEZoTmLXNTH3HS02nRARI9ehmNfFEJs2dv0i8axOvvjyenm1Wq37zjmZldUrVV3JaKStFyJCFPHOAQAyJs6ElMmGFDWpIhpiN7TNsMS/5J2aqgr8jvdF5XcP7iDh89Q+HW7+7cvfsvJXqe3QKuK9on4wnb0zrr45nx/7sgSUwoVRYYWkQgQJo0ITuz4OSDljamKvA/4IpACHEGZSzic7q8noYpyeLZubq+vY9wXKDehnwwIAAAwAAsJhVf/B4eEfHR3fk1A7QgdUVMRsZuPZHOqKdmdDSpuL66Zp8K94Nqvr2Ww2H1fTg7sfLtc/X7683uQfpNnOevMPw+m++k/yojcVsGQGABEMIX9rd+eHjx4/qiel5yq4Yj7V2g+aul6Xa+26tFrc5pyllvGrR0evPjgEsVPV5zokrjDkk371efcywfAe7D72E1aoFPJ8HP/w259enJXLVs7OL5+dZzvXWqbB17uzcm/uxsV4Z6eNtxc3V7HrEUDqndnuwb55d3Zz9b9qFy7sg3u4aiapTT6821QPuQTlCchUqQuzxe9+R+Ct7uJ2+ezT+Mnp5Co2l8svebDzrzxdF69MX/9u/fjbb93vl5vLxXq9ljfevMcFquDDt95ePXuhZ9ez22a8aCfqeppXVdFrSqqAPFHAlzf4wSfv/sl7/YO93zSrJ//3ZD317/fTj3D5WUjvxKLM/PH/PPnxd9548/238qZdrlby/p/+wHlfTSe5sxf//rPx8wWTJrRMAAYD45nL/7x4PqXiz8rX7nfEv3rWLjpJetRcvbqoL7vhNahmfvQvu8P4qntzevjB5dN/+uW/Pvzj7099GJvJf0I6fOVOtByX15uurwFW2K8htpgj5KJpf43LD6q2gjiN5z+iu/WwvnN+Y1/dpNXibRalAinuDXK76w++/7a/tu/RQ/ht/9+/+Ki/P3/69Ck+ODre2dlB0KN6snu6eO06A/S9xmyqoAn4l9ysvvtG3HS7T05/Co8eq5+GcpG7Exg88EFSQrxG+cV79aO//vMXJ2dV5PnZ+m9/9fPPfWo3jUTw59frgjm3t1fS3Uo7GlpREzUjvVHggzu/9+73/uPDDy8DnvbdPSss2ZMa/i6sLiT9zlU8Vv/3cdGvjy5//V9tvw5Ad07WJ5dXp9BrypLYs6ME2Ka8/+DQ2s+6tmFAh2igBLQ7mviqvnt0/+KL09O2fQeqWaIjdXuuPsHVM0skk6f5Rfubp8/+5tMxqll+Ayb98dRZsVmvJaYEIiZs3ulo9JKgEFeIsILLUczNQv3o7vEnH328apvPwE5dmIL5Rn+/GV43WloZcvwLd4R16DWnVb8opHPwHIwInXOSU/bOISMRXSw2X/S9FFIXJcQMXQLtx+ur0dlJXVVR88d2k1LzjygrSws2rHxRFN2mXQ8DtBTNdu8eclkPkM0ZJxURQUJEFEAX7eUX5+uIYHDTri3nbAOAubMvvnF5sTubi3cvcncxbAaABEaKYYgFDCnHLg2q4Mry0kUHTWbwAwo75xzuHr7uvXeMPmurioQCmlNOceiHLsZhGOLB0XGK6cXZKYBuAxgCMIB+HceQDAARiCATkUBwtS+m+3vCJMyMRIgIgXdCCMEjGRERMxuAWs55tVqt4ure4TGzIGKMMaWUNSMSEzEzIhIRABg6cYWIQ4LIWYcozjnnXHBSehHnRJgFQwjMzIBCTET7+/sAICLOuZyzqprZ9jtLKSF+nVZVNRsqoKr12uX1sslZmNk5571zXpxzLCyCzjkRIQMhZmYAQMQtSkppeyUiM+v7fjvPzHLOGSybxRh1yI+ryWluJAQfQvCOnRPnnAgTI2/LkImIaCu3PXjvt/hb8K0zOedtM1sGREDbSfqtFomCFIXznr3IFlZEfJDtE2wtMjMA2BptZlvwrfp2XkqJiLZ7QAYzA5Sj8c7h1IP6/wepR/IwCWZ6FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b24c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AugMix', 'AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose', 'ConvertImageDtype', 'ElasticTransform', 'FiveCrop', 'GaussianBlur', 'Grayscale', 'InterpolationMode', 'Lambda', 'LinearTransformation', 'Normalize', 'PILToTensor', 'Pad', 'RandAugment', 'RandomAdjustSharpness', 'RandomAffine', 'RandomApply', 'RandomAutocontrast', 'RandomChoice', 'RandomCrop', 'RandomEqualize', 'RandomErasing', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomInvert', 'RandomOrder', 'RandomPerspective', 'RandomPosterize', 'RandomResizedCrop', 'RandomRotation', 'RandomSolarize', 'RandomVerticalFlip', 'Resize', 'TenCrop', 'ToPILImage', 'ToTensor', 'TrivialAugmentWide', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_pil_constants', '_presets', 'autoaugment', 'functional', 'functional_pil', 'functional_tensor', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(  dir(transforms)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35e83868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x186C2078908>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(img_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6664c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "img_train_tr = to_tensor( img_train )\n",
    "\n",
    "print(  img_train_tr.shape  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f660eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensor_cifar10_training = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ec26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_tr, label = tensor_cifar10_training[23456]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec1c407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7412, 0.6314, 0.6627,  ..., 0.5922, 0.8471, 0.9922],\n",
      "         [0.4588, 0.2392, 0.3020,  ..., 0.1647, 0.6431, 0.9608],\n",
      "         [0.4392, 0.2000, 0.2667,  ..., 0.1451, 0.5137, 0.8824],\n",
      "         ...,\n",
      "         [0.8392, 0.7725, 0.7882,  ..., 0.7255, 0.6196, 0.8118],\n",
      "         [0.8863, 0.7294, 0.6627,  ..., 0.6275, 0.6196, 0.8353],\n",
      "         [0.9412, 0.7608, 0.6627,  ..., 0.6588, 0.7216, 0.8902]],\n",
      "\n",
      "        [[0.7725, 0.6863, 0.7137,  ..., 0.5961, 0.8353, 0.9804],\n",
      "         [0.5137, 0.3412, 0.4235,  ..., 0.1843, 0.6353, 0.9490],\n",
      "         [0.5020, 0.3333, 0.4275,  ..., 0.1804, 0.5137, 0.8745],\n",
      "         ...,\n",
      "         [0.8235, 0.7490, 0.7725,  ..., 0.7255, 0.6196, 0.8196],\n",
      "         [0.8706, 0.6980, 0.6431,  ..., 0.6157, 0.6157, 0.8392],\n",
      "         [0.9333, 0.7451, 0.6549,  ..., 0.6549, 0.7176, 0.8902]],\n",
      "\n",
      "        [[0.7686, 0.6745, 0.6784,  ..., 0.5882, 0.8353, 0.9843],\n",
      "         [0.5020, 0.3020, 0.3373,  ..., 0.1843, 0.6431, 0.9569],\n",
      "         [0.4941, 0.2784, 0.3216,  ..., 0.1882, 0.5333, 0.8824],\n",
      "         ...,\n",
      "         [0.7765, 0.6706, 0.6980,  ..., 0.6471, 0.5882, 0.8118],\n",
      "         [0.8510, 0.6627, 0.6000,  ..., 0.5882, 0.6157, 0.8471],\n",
      "         [0.9294, 0.7333, 0.6392,  ..., 0.6510, 0.7294, 0.8980]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(img_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41986a69",
   "metadata": {},
   "source": [
    "\n",
    "## Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6601e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs_list = [img_t for img_t, label in tensor_cifar10_training]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de191dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(imgs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a273a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_imgs_tr = torch.stack(  imgs_list, dim=3   )\n",
    "\n",
    "print( all_imgs_tr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93c36906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51200000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "view1 = all_imgs_tr.view(3, -1)\n",
    "print(  view1.shape  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e1ff9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "view1_mean = view1.mean(dim=1)\n",
    "print(view1_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eb2cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "view1_std = view1.std(dim=1)\n",
    "print(view1_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72f726ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## pixel = ( pixel - mean) / standard_dev\n",
    "\n",
    "transformed_cifar10_train = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                                               transform=transforms.Compose([\n",
    "                                                     transforms.ToTensor(),\n",
    "                                                     transforms.Normalize(view1_mean, view1_std)\n",
    "                                            ]))\n",
    "\n",
    "\n",
    "transformed_cifar10_test = datasets.CIFAR10(data_path, train=False, download=False,\n",
    "                                               transform=transforms.Compose([\n",
    "                                                     transforms.ToTensor(),\n",
    "                                                     transforms.Normalize(view1_mean, view1_std)\n",
    "                                            ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "179ce0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformed_cifar10_test.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac70e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 32 x 32 x 3 = 3072 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2912c",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23a0bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_map = {0:0, 2:1 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8920290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cifar10_final_train = [  (img, label_map[label])  for img, label in transformed_cifar10_train if label in [0, 2]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6d34a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cifar10_final_test = [  (img, label_map[label])  for img, label in transformed_cifar10_test if label in [0, 2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02ae12",
   "metadata": {},
   "source": [
    "\n",
    "## Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29a82125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2a6123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_DataLoader = torch.utils.data.DataLoader(cifar10_final_train, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76913ded",
   "metadata": {},
   "source": [
    "\n",
    "## Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a87a791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## you need to convert these to object oriented format\n",
    "\n",
    "model_mlp = nn.Sequential(\n",
    "      nn.Linear( 3072, 512),\n",
    "      nn.ReLU(),     ## Tanh(). sigmoid(), etc. \n",
    "      nn.Linear(512, 2),\n",
    "      nn.Softmax(dim=1)\n",
    "\n",
    ")\n",
    "\n",
    "## img ,  3     ->> [    ]     one_hot_encoding  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Deep Learning\n",
    "\n",
    "model_3DL = = nn.Sequential(\n",
    "      nn.Linear( 3072, 1024),\n",
    "      nn.ReLU(),    \n",
    "      nn.Linear(1024, 512),\n",
    "      nn.ReLU(), \n",
    "      nn.Linear(512, 128),\n",
    "      nn.ReLU(), \n",
    "      nn.Linear(128, 2),\n",
    "      nn.LogSoftmax(dim=1)\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_fn = model_3DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29346abf",
   "metadata": {},
   "source": [
    "\n",
    "## Training process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "optimizer = optim.Adam(  model_fn.parameters() , lr=learning_rate  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05b86f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5628, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5007, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5617, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3133, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5008, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_DataLoader:\n",
    "        ##print(labels)\n",
    "        ## resize img\n",
    "        imgs_resized = imgs.view(imgs.shape[0], -1 )\n",
    "        preds = model_fn(imgs_resized)\n",
    "        loss = loss_fn(  preds, labels  )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe431a",
   "metadata": {},
   "source": [
    "\n",
    "## Test model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16e05198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len( cifar10_final_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd839f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_DataLoader = torch.utils.data.DataLoader(  cifar10_final_test, batch_size=2000, shuffle=False  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b158c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_DataLoader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_fn(  imgs.view(batch_size, -1)   )\n",
    "        vals, indeces = torch.max( outputs, dim=1  )\n",
    "        preds = indeces\n",
    "        metric = (preds == labels).sum()\n",
    "\n",
    "        total = imgs.shape[0]\n",
    "\n",
    "result = metric / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "969afeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7435)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4404e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_metrics_function(y_test, y_pred):\n",
    "    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confmat)\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a215db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Confusion Matrix:\n",
      "[[712 288]\n",
      " [225 775]]\n",
      "Precision: 0.744\n",
      "Recall: 0.744\n",
      "F1-measure: 0.743\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_DataLoader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_fn(  imgs.view(batch_size, -1)   )\n",
    "        vals, indeces = torch.max( outputs, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function(labels, preds)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfd38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2   -> [0, 0, 1, 0, 0 , 0, 0, 0, 0 , 0]\n",
    "\n",
    "## 7   -> [0, 0, 0, 0, 0 , 0, 0, 1, 0 , 0]\n",
    "\n",
    "## 4   -> [0, 0, 0, 0, 1, 0, 0, 0, 0 , 0]  <>  [0 ,1 ,0, , , , , , , , ]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cec5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(  [0.4, 0.7, 0.5]  )\n",
    "\n",
    "print(   my_softmax(x).sum()  )\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e6bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384e800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e26246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101e1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b2748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b59b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9b181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
