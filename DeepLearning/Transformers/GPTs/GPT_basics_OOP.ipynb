{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80663990-d914-45f9-95ed-fe3ffcbf881d",
   "metadata": {},
   "source": [
    "\n",
    "## Simple GPT implementation in Torch\n",
    "\n",
    "The GPT uses the decoder only part of the Transformer.\n",
    "\n",
    "The input to the decoder varies based on whether you are training or predicting. If you are training, the input to the decoder is the sentence itself. When training, a mask is needed here to prevent the model from seeing all the words it is trying to predict. This is called a look ahead mask.  \n",
    "\n",
    "If you are testing, the input is just the previous words before the word you are trying to predict. You start with a start of sentence token (e.g. <sos>) and predict. The predicted word is then added to the previous tokens and the process is repeated. \n",
    "\n",
    "The decoder consists of N (e.g. 6) decoder layers, followed by a linear layer. \n",
    "\n",
    "Each decoder layer has a decoder multi-head attention layer, followed by a fully connected layer. \n",
    "\n",
    "The attention layers consist of N (e.g. 8) parallel attention sub layers that are later concatenated. \n",
    "\n",
    "The numbers 6 and 8 are a choice the architect makes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cef84-68fd-48e9-a632-216b8e39a347",
   "metadata": {},
   "source": [
    "\n",
    "Transformers are the latest step in the evolution of deep learning. As is necessary with progress, newer deep learning algorithms are also much more complicated, with deeper and more resource intensive networks. The Transformer is the best example of this. Transformers are, for me, the first algorithm I was not able to run on a laptop. They truly require a machine learning “war machine”. Lots of GPU power and memory, etc. The algorithms are much more complicated and, as you well see, the networks are very deep.\n",
    "Transformers are one of the new innovations in NLP since 2017. They were first made popular by the paper “Attention Is All You Need” by Vaswani et al. (2017). They are very interesting and seem to be very powerful. Many researchers suggests that they are better than RNNs for NLP because they parallelize better and because of the Attention mechanism.\n",
    "So far, Transformers have been used to develop very impressive implementations such as BERT (Devlin et al. 2018), and GPT-2 (Radford et al. 2019), as of this writing, which seem to be very good at language understanding. Transformers have been applied to language translation, question answering, document summarization, automatic code generation, text generation, etc. Okay, let’s get started.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d25d4-2293-4de8-86a8-eeee744d2432",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder Decoder with Multi-Head Attention\n",
    "\n",
    "In this section, I will present the first version of the Transformer first made popular in the paper “Attention Is All You Need” by Vaswani et al. As of this writing, there are newer versions of Transformers called BERT, GPT-2, GPT-3, etc. I will simply call this first implementation the Encoder Decoder with Multi Head Attention\n",
    "\n",
    "Transformer (that’s a mouth full). The Encoder Decoder with Multi Head Attention Transformer is a very deep network. The architecture has an encoder followed by a decoder. The encoder has 6 sublayers called encoder layers.\n",
    "Each encoding layer has a Multi-Head Attention layer followed by a standard fully connected feed forward layer. The input to the encoder goes through all this layers in the encoder an is converted into an encoder output. The input to the encoder and the output of the encoder have the same dimensions. For instance, here, the input to the encoder would be the English sentence (given a translation problem).\n",
    "The decoder layer has 2 inputs. One input is the encoder output. The second input to the decoder varies based on whether you are training or predicting. If you are training, the input to the decoder is the sentence in the other language. For instance, the Spanish sentence. In the decoder, when training the Transformer, a mask is needed to prevent the model from seeing all the words it is trying to predict. This is called a look ahead mask.\n",
    "If you are testing, the input to the decoder is just the previous words before the word you are trying to predict. You start with a start of sentence token (e.g. <sos>) and predict iteratively. The predicted word is then added to the previous tokens and the process is repeated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9102748-be63-4c30-b78b-0347c4b8bf91",
   "metadata": {},
   "source": [
    "\n",
    "![alternative text](full_transformer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998612ea-b6f5-4f60-a422-cc23f66a7732",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have looked at the big picture, we can proceed to discuss the main ideas of the Transformer model.\n",
    "\n",
    "\n",
    "## The Main Ideas of the Transformer\n",
    "\n",
    "So, where does one start with Transformers? The Transformer is complex and it involves several ideas to make them work correctly. In this section I will present the main ideas first with some relevant code. Understanding these concepts or steps really well before venturing to write the code for the whole Transformer is really important. It will save you time in the long run. So now, let us proceed to discuss these topics. In the next section I will start discussing the code for the the full transformer.\n",
    "Numpy arrays, tensors, and linear algebra\n",
    "Linear algebra, numpy arrays, and tensor operations are at the heart of understanding the Transformer architecture. Before you continue, I strongly recommend that you read and practice the topics in chapter 1, and in particular, the section on linear algebra, numpy arrays, and tensor operations.\n",
    "Inputs and outputs\n",
    "When dealing with deep neural networks I like to think of inputs and outputs first and treat the network as a black box. So, let us start there. Let's quickly remember our classic example of MNIST supervised classification. In MNIST standard feed forward classification, you have an input image which is 28x28 and a predicted vector of size 10 for the classes. So, what do the inputs and outputs look like for transformers? For language translation, they are lists of ids. Each id can represent a word in a sentence. This is best visualized with an example.\n",
    "\n",
    "First, let us look at the classic use case for Transformers. As I said earlier, Transformers have been used extensibly in NLP. And the simplest example is language translation where we have sentence pairs. Such as the following for English-Spanish translation:\n",
    "\"the cat is sleeping\" --> which translates to -- > \"el gato esta durmiendo\"\n",
    "Therefore, first we need to understand how to encode this for the neural network and then to understand how exactly it is that the network will train and learn. So, again, before you look into the network's very deep and complex layers, I believe that one needs to focus on:\n",
    "*Taking text sentences and converting them into sequences of ids\n",
    "*Padding these sequences of ids\n",
    "Consider that after encoding and padding, your sentences will look like this: \n",
    "\n",
    "English\n",
    "\n",
    "\n",
    "[12110 203 43947 29 2 168 2 4 27 684333 836222943 1012 112111 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "\n",
    "Spanish\n",
    "\n",
    "[12110 13 43947 29 2 5 32 36 161145 458 347905 58 25 28 354 2482 3 17 27 28 4395 9 2886 7 12111 0 0 0 0 0 0 0 0 0 0 0]\n",
    "\n",
    "\n",
    "## Masks\n",
    "Masks serve several purposes. One is to help ignore the padded values during training. The other goal is to block the given word you want to predict (or future words). This brings up the important aspect of training with Transformers. Transformers predict the last word in a sequence. For example:\n",
    "Given an input in english: \"the cat is sleeping\"\n",
    "a Transformer is also given part of the output sentence. In this case: \"el gato esta ?\". The Transformer will predict the next word in the sequence which in this case would be \"durmiendo\" to complete the translation as “el gato esta durmiendo”. All of this is achieved through the masks to ignore padded values and to only show the partial sentence. The type of training that will be used for Transformer training is called “Teacher Forcing”. So definitely understand this concept.\n",
    "Teacher forcing\n",
    "You may have already read somewhere (on-line) that the Transformer network predicts one word at a time and that that word is read back as an input in the next iteration. Also, the network predicts the last word in the sequence of words. But you may think, aren't those last words just padding? Eh? So, what is going on here? As it turns out, the mechanism of predicting one word at a time and feeding it back as an input in the next iteration is only done during the testing phase and it is not done during training. Instead, during training we use “Teacher Forcing”.\n",
    "Teacher forcing is a technique in auto regressive models where you do not use the predicted outputs of the decoder to feed back as input but instead you use the real data. This helps the model to learn the correct information instead of its own erroneous predictions (especially at the beginning of training).\n",
    "\n",
    "\n",
    "## Attention\n",
    "The Attention mechanism in Transformers is the heart of the whole algorithm. The attention matrix is nothing more than a dot product matrix multiplication between all the words in a sentence (e.g. the input English sentence). The idea is that, given the input and output, the model learns to correlate the words in the sentence to determine their importance. This is done multiple times and that is why it is called a multi head attention mechanism.\n",
    "\n",
    "## Embeddings\n",
    "Embedding converts the sequence of ids into a sequence of embeddings. You will go from a 2d tensor to a 3d tensor of size:\n",
    "N * seq_length_max * embedding_dimension\n",
    "where N is the batch size, seq_length_max is 40, and embedding_dimension is 512.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db03c935-38b7-4fee-b8fc-bc14367e56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "## import tiktoken\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a48ae69b-540a-417f-b34c-7776ac4ef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## !pip install requests\n",
    "## !pip install tiktoken    ## requires python   >    3.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e8786b6-250b-4066-a0f1-35f07c88db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 256      ## max content length for predictions\n",
    "batch_size = 64 \n",
    "max_iters  = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4             ## 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "vocab_size = 65\n",
    "n_embd  = 384                  ## every id gets embedded to vector of this size\n",
    "n_head  = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56ecaeac-06a8-4952-86b9-34a7643db327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file_path = 'input.txt'\n",
    "\n",
    "## data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c5310e5-0493-4f57-b956-c2dcfa3f24a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data in characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"length of data in characters\")\n",
    "len(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd94aa3f-1afa-4c1b-b8fb-ce676a9ccd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'v',\n",
       " 'W',\n",
       " \"'\",\n",
       " '$',\n",
       " 'I',\n",
       " 'Q',\n",
       " 'L',\n",
       " ',',\n",
       " 'Y',\n",
       " 'w',\n",
       " 'D',\n",
       " 'e',\n",
       " 'P',\n",
       " 'h',\n",
       " 'z',\n",
       " 'F',\n",
       " 'n',\n",
       " 'l',\n",
       " 'T',\n",
       " '-',\n",
       " 'q',\n",
       " '&',\n",
       " 'p',\n",
       " '3',\n",
       " 'r',\n",
       " 'j',\n",
       " 'X',\n",
       " '!',\n",
       " 's',\n",
       " 'A',\n",
       " 'H',\n",
       " '\\n',\n",
       " 'O',\n",
       " '.',\n",
       " ':',\n",
       " 'S',\n",
       " 'K',\n",
       " 'C',\n",
       " 'N',\n",
       " 'E',\n",
       " 'Z',\n",
       " ' ',\n",
       " 'd',\n",
       " 'y',\n",
       " 'x',\n",
       " 'c',\n",
       " 'f',\n",
       " ';',\n",
       " '?',\n",
       " 'B',\n",
       " 'g',\n",
       " 'o',\n",
       " 'G',\n",
       " 'V',\n",
       " 'R',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'M',\n",
       " 'k',\n",
       " 'b',\n",
       " 'a',\n",
       " 'U',\n",
       " 'J']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    " list(set(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4489b21-261c-4f39-9237-237f5ce0b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chars = sorted(     list(set(text))   )\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(  ''.join(chars)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a14d90f-f165-4a9a-a7b4-358936be9497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12965a-a4f4-4811-9341-55fec7377ffe",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7c0ace4-e5f7-49c0-a11a-ffc840f5fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## tokenizer\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4fbbe37-7828-489d-9833-a8dedd4d7af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a387e322-5700-4574-878b-4ae05aa0ee03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'B',\n",
       " 15: 'C',\n",
       " 16: 'D',\n",
       " 17: 'E',\n",
       " 18: 'F',\n",
       " 19: 'G',\n",
       " 20: 'H',\n",
       " 21: 'I',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'b',\n",
       " 41: 'c',\n",
       " 42: 'd',\n",
       " 43: 'e',\n",
       " 44: 'f',\n",
       " 45: 'g',\n",
       " 46: 'h',\n",
       " 47: 'i',\n",
       " 48: 'j',\n",
       " 49: 'k',\n",
       " 50: 'l',\n",
       " 51: 'm',\n",
       " 52: 'n',\n",
       " 53: 'o',\n",
       " 54: 'p',\n",
       " 55: 'q',\n",
       " 56: 'r',\n",
       " 57: 's',\n",
       " 58: 't',\n",
       " 59: 'u',\n",
       " 60: 'v',\n",
       " 61: 'w',\n",
       " 62: 'x',\n",
       " 63: 'y',\n",
       " 64: 'z'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2459036-2ff8-4f2c-bb86-81e2d47d6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encode = lambda s: [ stoi[c]          for c in s   ]    ## encoder: string to integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51072fe0-ec8e-4b44-82dc-55978bdd384e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 39, 46, 46]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encode(\"bahh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91a2b318-258b-4cf0-aeef-34c36253f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decode = lambda l: ''.join(   itos[i] for i in l   )    ## decoder: interger to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cab9420-aa91-40a5-9f01-a308e875827d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahh'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decode([40, 39, 46, 46])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdf246-bc16-4e07-8856-cd88f07a4d09",
   "metadata": {},
   "source": [
    "\n",
    "## Encode the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17ede82d-e755-4ac1-8fb9-65f13b4c9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = torch.tensor(   encode(text), dtype=torch.long   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67309397-e8c8-4b20-ab99-0131d56fbbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85db0120-488f-433f-bab1-1f5c53b8591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n    = int(   0.9*len(data)   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b365c426-f3b4-49c4-a002-57af08243d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce973b7-a199-4c52-90c2-6cbf26b1fa4c",
   "metadata": {},
   "source": [
    "\n",
    "## Function to create batches\n",
    "\n",
    "* sentences are selected for x and y where they are the same but y is shifted by one from x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fa05c24-3d3d-4bc2-a4d0-14f7987e0e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([213173, 989153, 193174, 874116])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp_batch_size = 4\n",
    "temp_block_size = 16\n",
    "\n",
    "ix = torch.randint(   len(data) - block_size, (temp_batch_size,)   )\n",
    "ix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c23b9572-52ae-4bf8-bb70-ef8bee28e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59)\n",
      "tensor(43)\n",
      "tensor(58)\n",
      "tensor(17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index_temp in ix:\n",
    "    print(  data[index_temp]  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b06596f-3786-417a-b0a2-18bc28e50bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[59, 58,  1, 15, 50, 39, 56, 43, 52, 41, 43, 12,  1, 39, 52, 42],\n",
      "        [43, 56,  1, 24, 59, 41, 43, 52, 58, 47, 53,  8,  0,  0, 24, 33],\n",
      "        [58, 46, 53, 59, 45, 46, 58, 57,  6,  1, 39,  1, 50, 43, 45, 47],\n",
      "        [17, 37, 10,  0, 32, 56, 59, 50, 63,  6,  1, 57, 47, 56,  6,  1]])\n",
      "tensor([[58,  1, 15, 50, 39, 56, 43, 52, 41, 43, 12,  1, 39, 52, 42,  1],\n",
      "        [56,  1, 24, 59, 41, 43, 52, 58, 47, 53,  8,  0,  0, 24, 33, 15],\n",
      "        [46, 53, 59, 45, 46, 58, 57,  6,  1, 39,  1, 50, 43, 45, 47, 53],\n",
      "        [37, 10,  0, 32, 56, 59, 50, 63,  6,  1, 57, 47, 56,  6,  1, 47]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x  = torch.stack(    [  data[   i : i+   temp_block_size ]   for i in ix ]    ) \n",
    "y  = torch.stack(    [  data[ i+1 : i+1+ temp_block_size ]   for i in ix ]    )\n",
    "\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9f189f9-8296-4d7c-b6e0-c6f15040d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "        \n",
    "    ix = torch.randint(   len(data) - block_size, (batch_size,)   )\n",
    "    \n",
    "    x  = torch.stack(    [  data[   i : i+block_size ]     for i in ix ]    ) \n",
    "    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix ]    )\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a441fd-59d7-4e09-98b8-5ca15686cd10",
   "metadata": {},
   "source": [
    "\n",
    "## Estimate loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80fefd1f-f239-452b-b62c-89799fd7afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()    ## for efficiency\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()   ## no training\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()  ## back to training\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1defb00-d802-445c-8dac-b8947a510bd2",
   "metadata": {},
   "source": [
    "\n",
    "## One Head of Self attention\n",
    "\n",
    "The english sentence and corresponding padding mask are the only inputs to this attention layer.\n",
    "\n",
    "The output of this attention mechanism. The output of this  Attention mechanism is passed to a fully connected layer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "859a0e9f-9d5a-47ae-a972-d9ab8da4e8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n## x [N, 40, 512]\\n## look_ahead_mask [N, 40, 40]\\n\\ndef Dec_MultiHeadAttention(x, look_ahead_mask, dropout):\\n\\n    Wq = tf.Variable( xavier_init( [batch_size, 512, 64] )  )\\n    bq = tf.Variable( tf.random_normal( [batch_size, 40, 64] )  )\\n    Q = tf.matmul(x, Wq) + bq    # Nx40x64\\n    \\n    Wk = tf.Variable( xavier_init( [batch_size, 512, 64] )  )\\n    bk = tf.Variable( tf.random_normal( [batch_size, 40, 64] )  )\\n    K = tf.matmul(x, Wk) + bk    # Nx40x64\\n    \\n    Wv = tf.Variable( xavier_init( [batch_size, 512, 64] )  )\\n    bv = tf.Variable( tf.random_normal( [batch_size, 40, 64] )  )\\n    V = tf.matmul(x, Wv) + bv    # Nx40x64\\n    \\n\\n    ## calc a score of word_i importance to all other words\\n    scores_matrix = tf.matmul( Q, K, transpose_b=True)        ### [N, 40, 40]\\n    scores_matrix = scores_matrix/( tf.sqrt(64.0) )           ### [N, 40, 40]\\n    \\n    ################################# ## look_ahead_mask [N, 40, 40]\\n    ## [N, 40, 40] + [N, 40, 40]\\n    scores_matrix = scores_matrix + (look_ahead_mask * -1e9)\\n    ## [N, 40, 40]\\n    \\n    #################################\\n    # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. \\n    ## axis -1 is for last dimension in this tensor\\n    \\n    a1 = tf.nn.softmax(scores_matrix, axis=-1) # (N, seq_len_q, seq_len_k) a1 = tf.nn.dropout(a1, dropout)\\n    a2 = tf.matmul(a1, V) ## [N, 40, 40] * [N, 40, 64]\\n    \\n    return a2 ## [N, 40, 64]\\n    \\n\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "## x [N, 40, 512]\n",
    "## look_ahead_mask [N, 40, 40]\n",
    "\n",
    "def Dec_MultiHeadAttention(x, look_ahead_mask, dropout):\n",
    "\n",
    "    Wq = tf.Variable( xavier_init( [batch_size, 512, 64] )  )\n",
    "    bq = tf.Variable( tf.random_normal( [batch_size, 40, 64] )  )\n",
    "    Q = tf.matmul(x, Wq) + bq    # Nx40x64\n",
    "    \n",
    "    Wk = tf.Variable( xavier_init( [batch_size, 512, 64] )  )\n",
    "    bk = tf.Variable( tf.random_normal( [batch_size, 40, 64] )  )\n",
    "    K = tf.matmul(x, Wk) + bk    # Nx40x64\n",
    "    \n",
    "    Wv = tf.Variable( xavier_init( [batch_size, 512, 64] )  )\n",
    "    bv = tf.Variable( tf.random_normal( [batch_size, 40, 64] )  )\n",
    "    V = tf.matmul(x, Wv) + bv    # Nx40x64\n",
    "    \n",
    "\n",
    "    ## calc a score of word_i importance to all other words\n",
    "    scores_matrix = tf.matmul( Q, K, transpose_b=True)        ### [N, 40, 40]\n",
    "    scores_matrix = scores_matrix/( tf.sqrt(64.0) )           ### [N, 40, 40]\n",
    "    \n",
    "    ################################# ## look_ahead_mask [N, 40, 40]\n",
    "    ## [N, 40, 40] + [N, 40, 40]\n",
    "    scores_matrix = scores_matrix + (look_ahead_mask * -1e9)\n",
    "    ## [N, 40, 40]\n",
    "    \n",
    "    #################################\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. \n",
    "    ## axis -1 is for last dimension in this tensor\n",
    "    \n",
    "    a1 = tf.nn.softmax(scores_matrix, axis=-1) # (N, seq_len_q, seq_len_k) a1 = tf.nn.dropout(a1, dropout)\n",
    "    a2 = tf.matmul(a1, V) ## [N, 40, 40] * [N, 40, 64]\n",
    "    \n",
    "    return a2 ## [N, 40, 64]\n",
    "    \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93951915-1deb-43ea-bf75-02902312e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        ## the mask tril is not part of the graph since only for masking\n",
    "        ## so register buffer makes it a thing out of the graph\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)              ## (B, T, C)\n",
    "        q = self.query(x)            ## (B, T, C)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5       ## (B, T, C) @ (B, C, T)  -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))     ## (B, T, T)\n",
    "        wei = F.softmax(wei, dim= -1)           ## (B, T, T)\n",
    "        wei = self.dropout(   wei   )\n",
    "        \n",
    "        ## perform the weighted aggregation of the values\n",
    "        v   = self.value(  x  )   ## (B, T, C)\n",
    "        out = wei @ v             ## (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        \n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef1c29-294a-49d4-98d5-4503884c1ab6",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "\n",
    "The Masked multi-head attention layer is done N (e.g. 8) times in parallel  and the results are concatenated. \n",
    "\n",
    "This concatenated result is added to the original after mapping it through one more layer to calculate the residual. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46012e1-6fd7-4ea0-99e6-dd49cf75c064",
   "metadata": {},
   "source": [
    "\n",
    "![alternative text](encoder_layer.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eeb74948-7b70-414a-adb0-2bc689f55d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n## input_dec_layer = [N, 40, 512]\\n\\n## dec_look_ahead_comb_mask [N, 40, 40]\\n\\n\\ndef decoder_layer(input_dec_layer,dec_look_ahead_comb_mask, dropout):\\n\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_1\"):\\n        z1 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_2\"):\\n        z2 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_3\"):\\n        z3 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_4\"):\\n        z4 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_5\"):\\n        z5 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_6\"):\\n        z6 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_7\"):\\n        z7 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Dec_MultiHead_Attention_8\"):\\n        z8 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\\n        \\n    z_concat = tf.concat([z1, z2 ,z3, z4, z5, z6, z7, z8], -1) ## [N, 40, 512]\\n    \\n    W0 = tf.Variable( xavier_init( [batch_size, 8*64, 512] ) ) \\n    b0 = tf.Variable( tf.random_normal( [batch_size, 40, 512] ) ) \\n    z1 = tf.matmul(z_concat, W0) + b0\\n    \\n    residual1 = layer_norm(input_dec_layer + z1)\\n    \\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "## input_dec_layer = [N, 40, 512]\n",
    "\n",
    "## dec_look_ahead_comb_mask [N, 40, 40]\n",
    "\n",
    "\n",
    "def decoder_layer(input_dec_layer,dec_look_ahead_comb_mask, dropout):\n",
    "\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_1\"):\n",
    "        z1 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_2\"):\n",
    "        z2 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_3\"):\n",
    "        z3 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_4\"):\n",
    "        z4 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_5\"):\n",
    "        z5 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_6\"):\n",
    "        z6 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_7\"):\n",
    "        z7 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Dec_MultiHead_Attention_8\"):\n",
    "        z8 = Dec_MultiHeadAttention(input_dec_layer, dec_look_ahead_comb_mask, dropout)\n",
    "        \n",
    "    z_concat = tf.concat([z1, z2 ,z3, z4, z5, z6, z7, z8], -1) ## [N, 40, 512]\n",
    "    \n",
    "    W0 = tf.Variable( xavier_init( [batch_size, 8*64, 512] ) ) \n",
    "    b0 = tf.Variable( tf.random_normal( [batch_size, 40, 512] ) ) \n",
    "    z1 = tf.matmul(z_concat, W0) + b0\n",
    "    \n",
    "    residual1 = layer_norm(input_dec_layer + z1)\n",
    "    \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06782df4-bd44-4bd0-a092-a20a57b52d67",
   "metadata": {},
   "source": [
    "\n",
    "In this part of the code in the Dec_Multihead_Attention function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "824de687-0703-4b7b-8542-3ba5a729c08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nWq = tf.Variable( xavier_init( [batch_size, 512, 64] ) ) \\nbq = tf.Variable( tf.random_normal( [batch_size, 40, 64] ) ) \\nQ = tf.matmul(x, Wq) + bq # Nx40x64\\n\\nWk = tf.Variable( xavier_init( [batch_size, 512, 64] ) ) \\nbk = tf.Variable( tf.random_normal( [batch_size, 40, 64] ) ) \\nK = tf.matmul(x, Wk) + bk # Nx40x64\\n\\nWv = tf.Variable( xavier_init( [batch_size, 512, 64] ) ) \\nbv = tf.Variable( tf.random_normal( [batch_size, 40, 64] ) ) \\nV = tf.matmul(x, Wv) + bv # Nx40x64\\n\\n\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Wq = tf.Variable( xavier_init( [batch_size, 512, 64] ) ) \n",
    "bq = tf.Variable( tf.random_normal( [batch_size, 40, 64] ) ) \n",
    "Q = tf.matmul(x, Wq) + bq # Nx40x64\n",
    "\n",
    "Wk = tf.Variable( xavier_init( [batch_size, 512, 64] ) ) \n",
    "bk = tf.Variable( tf.random_normal( [batch_size, 40, 64] ) ) \n",
    "K = tf.matmul(x, Wk) + bk # Nx40x64\n",
    "\n",
    "Wv = tf.Variable( xavier_init( [batch_size, 512, 64] ) ) \n",
    "bv = tf.Variable( tf.random_normal( [batch_size, 40, 64] ) ) \n",
    "V = tf.matmul(x, Wv) + bv # Nx40x64\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741e598-d257-4100-83b6-4b5cccea7e7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " You calculate the keys, queries, and values which are tensors that map the input x of size [N, 40, 512] to size [N, 40, 64]. We then calculate the scores matrix which is the Attention mechanism. This is a dot product. We matrix multiply Q with the transpose of K. This results in a matrix that is size [N, 40, 40].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b656991f-cb5d-4cbc-a9cf-a0dcc3447ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n## calc a score of word_i importance to all other words\\n\\nscores_matrix = tf.matmul( Q, K, transpose_b=True)     ### [N, 40, 40]\\nscores_matrix = scores_matrix/(tf.sqrt(64.0))          ### [N, 40, 40]\\n\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "## calc a score of word_i importance to all other words\n",
    "\n",
    "scores_matrix = tf.matmul( Q, K, transpose_b=True)     ### [N, 40, 40]\n",
    "scores_matrix = scores_matrix/(tf.sqrt(64.0))          ### [N, 40, 40]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496baa7-b2b9-4aaa-9834-bd19611c0ccd",
   "metadata": {},
   "source": [
    "\n",
    "After calculating the score matrix, we need to mask the values so that we don’t cheat by looking ahead. We apply the look ahead and padding masks. The mask for look ahead attention happens before the softmax calculation. Notice that the masking is done to the dot_product scores matrix only. The mask is multiplied with -1e9 (close to negative infinity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c015ffb5-7e81-44c5-bc6f-2a3c90e42c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n## look_ahead_mask [N, 40, 40]\\n## [N, 40, 40] + [N, 40, 40]\\n\\nscores_matrix = scores_matrix + (look_ahead_mask * -1e9) ## [N, 40, 40]\\n\\n\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "## look_ahead_mask [N, 40, 40]\n",
    "## [N, 40, 40] + [N, 40, 40]\n",
    "\n",
    "scores_matrix = scores_matrix + (look_ahead_mask * -1e9) ## [N, 40, 40]\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d43f42-46b9-4d23-807a-1e23d4fc04fb",
   "metadata": {},
   "source": [
    "\n",
    "This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out padded cells, and large negative inputs to softmax are near zero in the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5830b491-55c1-4e8d-8c1a-44c3367f85f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFor example, softmax for “a”\\n\\na = tf.constant([0.6, 0.2, 0.3, 0.4, 0, 0, 0, 0, 0, 0]) \\n\\ntf.nn.softmax(a)\\n\\ngives the following\\n\\n\\n<tf.Tensor: shape=(10,), dtype=float32, numpy=\\narray([0.15330984, 0.10276665, 0.11357471, 0.12551947, 0.08413821,\\n0.08413821, 0.08413821, 0.08413821, 0.08413821, 0.08413821], dtype=float32)>\\n\\nnow, if some of the values are negative infinities\\n\\nb = tf.constant([0.6, 0.2, 0.3, 0.4, -1e9, -1e9, -1e9, -1e9, -1e9, -1e9])\\n\\ntf.nn.softmax(b)\\n\\nthen softmax gives us\\n\\n<tf.Tensor: shape=(10,), dtype=float32, numpy=\\narray([ 0.3096101 , 0.20753784, 0.22936477, 0.25348732, 0. ,0. , 0. , 0. , 0. , 0. ], dtype=float32)>\\n\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "For example, softmax for “a”\n",
    "\n",
    "a = tf.constant([0.6, 0.2, 0.3, 0.4, 0, 0, 0, 0, 0, 0]) \n",
    "\n",
    "tf.nn.softmax(a)\n",
    "\n",
    "gives the following\n",
    "\n",
    "\n",
    "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
    "array([0.15330984, 0.10276665, 0.11357471, 0.12551947, 0.08413821,\n",
    "0.08413821, 0.08413821, 0.08413821, 0.08413821, 0.08413821], dtype=float32)>\n",
    "\n",
    "now, if some of the values are negative infinities\n",
    "\n",
    "b = tf.constant([0.6, 0.2, 0.3, 0.4, -1e9, -1e9, -1e9, -1e9, -1e9, -1e9])\n",
    "\n",
    "tf.nn.softmax(b)\n",
    "\n",
    "then softmax gives us\n",
    "\n",
    "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
    "array([ 0.3096101 , 0.20753784, 0.22936477, 0.25348732, 0. ,0. , 0. , 0. , 0. , 0. ], dtype=float32)>\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe5246-f4df-41c5-9afb-0dfc26d32fbb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice the infinities are now zeros! At this point, just like with the encoder_multihead_attention function, the decoder_multihead_attention function takes the scores_matrix after adding the mask and applies the softmax. The softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1. The value of axis = -1 is for the last dimension in this tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd8ea502-96fe-4fc6-8322-df0a5d133255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\na1 = tf.nn.softmax(scores_matrix, axis=-1) # (N, seq_len_q, seq_len_k) a1 = tf.nn.dropout(a1, dropout)\\na2 = tf.matmul(a1, V) ## [N, 40, 40] * [N, 40, 64]\\nreturn a2 ## [N, 40, 64]\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "a1 = tf.nn.softmax(scores_matrix, axis=-1) # (N, seq_len_q, seq_len_k) a1 = tf.nn.dropout(a1, dropout)\n",
    "a2 = tf.matmul(a1, V) ## [N, 40, 40] * [N, 40, 64]\n",
    "return a2 ## [N, 40, 64]\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e4c42-640f-4cb3-b897-68c57a3786a6",
   "metadata": {},
   "source": [
    "\n",
    "Finally, just like before, dropout is applied and the result is multiplied with the matrix V. The final tensor is of size [N, 40, 64].\n",
    "Remember that the previous function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c8ab998-0ac0-49bd-8768-d54e64964015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(  [ Head(head_size) for _ in range(num_heads) ] )\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat(   [ h(x) for h in self.heads ], dim = -1   )\n",
    "        out = self.proj(  out   )\n",
    "        out = self.dropout(   out   )\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31035505-c27e-421b-8b12-c8cd509ec167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c33b40-4f95-4f72-a5f8-dfcb73a1b082",
   "metadata": {},
   "source": [
    "\n",
    "## The N decoding blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3cf0af9-d389-495e-a126-f385c218900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: comuunication followed by computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward( n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## these normalizations (ln1, ln2) are about the only thing different from\n",
    "        ## the original Vaswani paper. In the paper, they are done at the end of forward\n",
    "        ## but now they are usually done at the beginning of forward\n",
    "        x = x + self.sa(     self.ln1(x)      )\n",
    "        x = x + self.ffwd(   self.ln2(x)      )\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb8c76-ea39-400b-92ec-56d7a4107ae5",
   "metadata": {},
   "source": [
    "\n",
    "## A GPT language model\n",
    "\n",
    "The decoder has one last layer as can be seen here\n",
    "\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "where h6 = [N,40,512]\n",
    "\n",
    "dec_out_one_hot = self.lm_head(h6)\n",
    "\n",
    "the returned dec_out_one_hot is of size [N, 40, vocabulary_size]\n",
    "\n",
    "this final layer maps a tensor of size [N, 40, 512] to a tensor of size [N, 40, vocab_size] where vocab_size is the size of the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6d1984f-5918-49f6-b958-df787246a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n## embed_pt_pos_dec_in = [N, 40, 512]\\n\\ndef decoder(embed_en_pos_dec_in,  dec_look_ahead_comb_mask, dropout):\\n\\n    with tf.variable_scope(\"Decoder_layer_1\"):\\n        h1 = decoder_layer(embed_en_pos_dec_in,  dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Decoder_layer_2\"): \\n        h2 = decoder_layer(h1,  dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Decoder_layer_3\"): \\n        h3 = decoder_layer(h2,  dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Decoder_layer_4\"): \\n        h4 = decoder_layer(h3,  dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Decoder_layer_5\"): \\n        h5 = decoder_layer(h4,  dec_look_ahead_comb_mask, dropout)\\n    with tf.variable_scope(\"Decoder_layer_6\"): \\n        h6 = decoder_layer(h5,  dec_look_ahead_comb_mask, dropout)\\n\\n\\n    ## h6 = [N,40,512]\\n\\n    dec_out_one_hot = dec_final_linear_layer(h6)\\n\\n    return dec_out_one_hot          ## [N, 40, vocabulary_size]\\n\\n\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "## embed_pt_pos_dec_in = [N, 40, 512]\n",
    "\n",
    "def decoder(embed_en_pos_dec_in,  dec_look_ahead_comb_mask, dropout):\n",
    "\n",
    "    with tf.variable_scope(\"Decoder_layer_1\"):\n",
    "        h1 = decoder_layer(embed_en_pos_dec_in,  dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Decoder_layer_2\"): \n",
    "        h2 = decoder_layer(h1,  dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Decoder_layer_3\"): \n",
    "        h3 = decoder_layer(h2,  dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Decoder_layer_4\"): \n",
    "        h4 = decoder_layer(h3,  dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Decoder_layer_5\"): \n",
    "        h5 = decoder_layer(h4,  dec_look_ahead_comb_mask, dropout)\n",
    "    with tf.variable_scope(\"Decoder_layer_6\"): \n",
    "        h6 = decoder_layer(h5,  dec_look_ahead_comb_mask, dropout)\n",
    "\n",
    "\n",
    "    ## h6 = [N,40,512]\n",
    "\n",
    "    dec_out_one_hot = dec_final_linear_layer(h6)\n",
    "\n",
    "    return dec_out_one_hot          ## [N, 40, vocabulary_size]\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ea7991d-baf4-4042-b7aa-01e4a0b58ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)     ## [vocab_size, embed_size]\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)     ## \n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n",
    "        )\n",
    "        self.ln_f    = nn.LayerNorm(  n_embd    )        ## final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        \n",
    "        ## ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)      ## batch, time, embed (4, 8, 32) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))      ## (T, C)\n",
    "        \n",
    "        x = tok_emb + pos_emb    ## (B, T, C)\n",
    "\n",
    "        ## This is the architecture\n",
    "        \n",
    "        x = self.blocks(  x  )   ## (B, T, C)        \n",
    "        x = self.ln_f(    x  )         ## (B, T, C)\n",
    "        \n",
    "        logits = self.lm_head(x)                 ## (B, T, vocab_sice)   ## logits are what is predicted\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C  = logits.shape\n",
    "            logits   = logits.view(B*T, C)\n",
    "            targets  = targets.view(B*T)\n",
    "            loss     = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        ## idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            ## crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            ## get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            ## focus only on last time stamp\n",
    "            logits = logits[:, -1, :]           ## becomes (B, C)\n",
    "            ## apply softmax to get probs\n",
    "            probs = F.softmax(logits, dim= -1)    ## (B, C)\n",
    "            ## sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1)\n",
    "            ## append sample to the running sequence\n",
    "            idx = torch.cat(  (idx, idx_next), dim=1  )            ## (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1916e-ebdd-4917-96b8-226e385802d8",
   "metadata": {},
   "source": [
    "\n",
    "The decoder has a final linear layer after the 6 decoder_layer functions. We proceed to discuss it in the next section.\n",
    "Decoder Final Linear Layer\n",
    "The final layer in the decoder is the decoder_final_layer. This is a linear layer with no non-linearities and a softmax that maps the tensor [N, 40, 512] to a tensor of size [N, 40, en_vocab_size] as can be seen in the next code segment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a621d143-6412-4a3e-9f85-06c33f71ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n## input = [N, 40, 512]\\n\\ndef dec_final_linear_layer(input):\\n    w_h1 = tf.Variable( xavier_init( [batch_size, 512, VOCAB_SIZE_EN]    ))\\n    b_h1 = tf.Variable( tf.random_normal(  [batch_size, 40, VOCAB_SIZE_PT] ))\\n    h1_mul = tf.matmul( input , w_h1 ) \\n    h1 = tf.add( h1_mul, b_h1 )\\n    \\n\\n    softmax_h1 = tf.nn.softmax( h1 , axis=-1 ) ## [N, 40, vocabulary_size] \\n    dec_out_one_hot = softmax_h1  ## [N, 40, vocabulary_size]\\n    \\n    #################\\n    ## if you wanted the ids, you could do this\\n    dec_out_ids = tf.argmax( softmax_h1 , axis=-1) \\n    dec_out_ids = tf.cast(dec_out_ids, tf.int32)\\n    \\n    #################\\n    ## you could return\\n    ## dec_out_ids or dec_out_one_hot\\n    ## [N, 40] [N, 40, vocabulary_size]\\n    ## because of the loss function used (sparse_cross_entropy) \\n    ## dec_out_one_hot seems to be the correct one\\n    \\n    return dec_out_one_hot ## [N, 40, vocabulary_size]\\n\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "\n",
    "## input = [N, 40, 512]\n",
    "\n",
    "def dec_final_linear_layer(input):\n",
    "    w_h1 = tf.Variable( xavier_init( [batch_size, 512, VOCAB_SIZE_EN]    ))\n",
    "    b_h1 = tf.Variable( tf.random_normal(  [batch_size, 40, VOCAB_SIZE_PT] ))\n",
    "    h1_mul = tf.matmul( input , w_h1 ) \n",
    "    h1 = tf.add( h1_mul, b_h1 )\n",
    "    \n",
    "\n",
    "    softmax_h1 = tf.nn.softmax( h1 , axis=-1 ) ## [N, 40, vocabulary_size] \n",
    "    dec_out_one_hot = softmax_h1  ## [N, 40, vocabulary_size]\n",
    "    \n",
    "    #################\n",
    "    ## if you wanted the ids, you could do this\n",
    "    dec_out_ids = tf.argmax( softmax_h1 , axis=-1) \n",
    "    dec_out_ids = tf.cast(dec_out_ids, tf.int32)\n",
    "    \n",
    "    #################\n",
    "    ## you could return\n",
    "    ## dec_out_ids or dec_out_one_hot\n",
    "    ## [N, 40] [N, 40, vocabulary_size]\n",
    "    ## because of the loss function used (sparse_cross_entropy) \n",
    "    ## dec_out_one_hot seems to be the correct one\n",
    "    \n",
    "    return dec_out_one_hot ## [N, 40, vocabulary_size]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035be312-9277-41b9-ad12-73bb85e131e6",
   "metadata": {},
   "source": [
    "\n",
    "## Instantiate GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d7c533f-c622-41ec-b5e6-314b01662181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model   = BigramLanguageModel()\n",
    "\n",
    "m = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda24346-d8ee-48bc-a81b-f0c7521920c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2a5ec-0893-4737-9e3a-83939cdda054",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860fd9c-c268-4f76-a9ca-0ecc04b4848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ## evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)   ## zero out\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcfc26-54ea-4050-8be4-93f79259a811",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Now, regenerate after some training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0e164-a918-44ad-8146-06c32df08789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Kick off generation with some starting token. In this case id 0\n",
    "\n",
    "context = torch.zeros(  (1, 1),  dtype=torch.long, device=device   )   ## scalar with value 0\n",
    "\n",
    "gen_text = m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "\n",
    "print(  decode(gen_text)   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4e842-f285-4aa4-994e-e206369e6eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1fda57-fdcf-43d4-8e2b-84f4adda088f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5bb67-ee2c-4593-9831-943f4760a89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0d614-5bec-4df8-acc8-041e25286438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0d0b6-efab-4348-95c0-033ecf9565ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
