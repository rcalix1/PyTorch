## Decoder only (GPTs)

## A Tiny GPT

* a tiny ChatGPT style LLM 

* took about 20 minutes training on my GPU

* GPT is only the decoder part of the original Transformer architecture (Vaswani et al. 2017)

* File: GenerativeTransformerTinyGPT.py

## Tokens

* The tokens are the characters 

 !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
 
## Train and test losses

step 0: train loss 4.2849, val loss 4.2823

step 500: train loss 2.0083, val loss 2.0968

step 1000: train loss 1.5968, val loss 1.7752

step 1500: train loss 1.4391, val loss 1.6376

step 2000: train loss 1.3421, val loss 1.5682

step 2500: train loss 1.2794, val loss 1.5312

step 3000: train loss 1.2278, val loss 1.5065

step 3500: train loss 1.1836, val loss 1.4900

step 4000: train loss 1.1469, val loss 1.4820

step 4500: train loss 1.1116, val loss 1.4772


## The tiny GPT speaking Shakespear :)

Such blest upon't: by their hostes' actions, 

The goals of either, though quiet is solder'd

She she blam thereod reasonary.

SICINIUS:

What, is't indeed? happed?

MENENIUS:

Come, brother, with myself and now

That a pronounds us may be accourse, they said.

When If you should love thee, Volscians; freeling suspecian.

Let thy lords so 'em'?' 'Forsel'? why have I tread?

SICINIUS:

I pray, you were th sword man's so parted train.

Show thy father, what news happy names

For his fraid pain ben of thy hol




